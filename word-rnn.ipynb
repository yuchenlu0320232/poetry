{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a text generator with Word-RNN \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path2 = r\"E:\\NLP project\\data\\PoetryFoundationData.csv\"\n",
    "df2 = pd.read_csv(file_path2, encoding='utf-8', quotechar='\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting sentences into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', sentence)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "df2['tokenized_poem'] = df2['Poem'].astype(str).apply(tokenize_sentence)\n",
    "\n",
    "df2['tokenized_poem'] = df2['Poem'].apply(tokenize_sentence)\n",
    "\n",
    "print(df2['tokenized_poem'].sample(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_roman_letters(text):\n",
    "    return re.sub(r'[IVXLCDMivxlcdm]', '', text)\n",
    "df2['cleaned_poem'] = df2['Poem'].astype(str).apply(remove_roman_letters)\n",
    "\n",
    "\n",
    "def remove_carriage_return(text):\n",
    "    return re.sub(r'\\r|\\n', ' ', text)\n",
    "df2['cleaned_poem'] = df2['cleaned_poem'].apply(remove_carriage_return)\n",
    "\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d', '', text)\n",
    "df2['cleaned_poem'] = df2['cleaned_poem'].astype(str).apply(remove_numbers)\n",
    "\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    text = remove_special_characters(text)\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "df2['cleaned_poem'] = df2['cleaned_poem'].apply(remove_carriage_return)\n",
    "\n",
    "\n",
    "print(df2['cleaned_poem'].head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install clean-text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "category = unicodedata.category('A')\n",
    "print(f'Category: {category}')\n",
    "\n",
    "numeric_value = unicodedata.numeric('1')\n",
    "print(f'Numeric Value: {numeric_value}')\n",
    "\n",
    "name = unicodedata.name('A')\n",
    "print(f'Name: {name}')\n",
    "\n",
    "is_digit = '9'.isdigit()\n",
    "print(f'Is Digit: {is_digit}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleantext import clean\n",
    "\n",
    "clean(\"some input\",\n",
    "    fix_unicode=True,               # fix various unicode errors\n",
    "    to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "    lower=True,                     # lowercase text\n",
    "    no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n",
    "    no_urls=True,                  # replace all URLs with a special token\n",
    "    no_emails=True,                # replace all email addresses with a special token\n",
    "    no_phone_numbers=True,         # replace all phone numbers with a special token\n",
    "    no_numbers=True,               # replace all numbers with a special token\n",
    "    no_digits=True,                # replace all digits with a special token\n",
    "    no_currency_symbols=True,      # replace all currency symbols with a special token\n",
    "    no_punct=True,                 # remove punctuations\n",
    "    replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
    "    replace_with_url=\"<URL>\",\n",
    "    replace_with_email=\"<EMAIL>\",\n",
    "    replace_with_phone_number=\"<PHONE>\",\n",
    "    replace_with_number=\"<NUMBER>\",\n",
    "    replace_with_digit=\"0\",\n",
    "    replace_with_currency_symbol=\"<CUR>\",\n",
    "    lang=\"en\"                       # set to 'de' for German special handling\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering to remove low-frequency words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "all_tokens2 = [token for tokens in df2['tokenized_poem'] for token in tokens]\n",
    "word_counts2 = Counter(all_tokens2)\n",
    "\n",
    "\n",
    "filtered_vocab2 = {word for word, count in word_counts2.items() if count > 10}\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nRandom 20 words from Vocabulary - df2:\")\n",
    "random_words2 = random.sample(filtered_vocab2, 20)\n",
    "print(random_words2)\n",
    "\n",
    "print(f\"Total unique tokens - df2: {len(filtered_vocab2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset size:\", len(df2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a glossary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vocab_file_path = \"E:/NLP project/data/vocab.txt\"\n",
    "with open(vocab_file_path, 'w', encoding='utf-8') as f:\n",
    "    for word in filtered_vocab2:\n",
    "        f.write(word + '\\n')\n",
    "\n",
    "print(f\"Vocabulary saved to: {vocab_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 10   # size of hidden state\n",
    "batch_size = 5    # size of the batch used for training\n",
    "step_len = 2     # number of training samples in each stem\n",
    "num_layers = 5      # number of layers in LSTM layer stack\n",
    "lr = 0.002          # learning rate\n",
    "num_steps = 20     # max number of training steps\n",
    "gen_seq_len = 10    # length of generated sequence\n",
    "load_chk = False    # load in pre-trained checkpoint for training\n",
    "save_path = \"word_rnn_model.pt\"\n",
    "# load_path = \"word_rnn_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_text_files_in_folder(path, max_files = 10000):\n",
    "    corpus = ''\n",
    "    # Find all files in the folder or subfolders\n",
    "    for root, _, files in os.walk(path):\n",
    "        for i, file in enumerate(files):\n",
    "            # If the file is a text file\n",
    "            if file.endswith(\".txt\") and i <= max_files:\n",
    "                # Open the file and add the text to the corpus\n",
    "                with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                    # Add text from file\n",
    "                    corpus += text\n",
    "                    # Add 'End of File' between documents\n",
    "                    corpus += '\\n EOF \\n'\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"E:/NLP-23-24-main/data/vocab.txt\"\n",
    "corpus = load_all_text_files_in_folder(data_path)\n",
    "words = sorted(list(set(corpus.split())))\n",
    "data_size, vocab_size = len(corpus.split()), len(words)\n",
    "\n",
    "vocab_to_index = {word: idx for idx, word in enumerate(filtered_vocab2)}\n",
    "df2['tokenized_poem_indices'] = df2['tokenized_poem'].apply(lambda tokens: [vocab_to_index[word] for word in tokens if word in vocab_to_index])\n",
    "print(df2['tokenized_poem_indices'].sample(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(vocab_size, vocab_size, hidden_size, num_layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_training_batch_indicies(index_list, batch_size):\n",
    "    \n",
    "    batch_size = min(batch_size, len(index_list))\n",
    "    \n",
    "    \n",
    "    input_batch_indices = torch.tensor(np.array(random.choices(index_list, k=batch_size)))\n",
    "    \n",
    "   \n",
    "    target_batch_indices = input_batch_indices + 1\n",
    "    \n",
    "    return input_batch_indices, target_batch_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset size:\", len(df2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(index_list) < step_len:\n",
    "    print(f\"Insufficient data for training. Dataset size: {len(index_list)}, Step length: {step_len}\")\n",
    "else:\n",
    "\n",
    "\n",
    "\n",
    "for step in range(1, num_steps):\n",
    "    running_loss = 0\n",
    "    hidden_state = None\n",
    "    rnn.zero_grad()\n",
    "    train_batch_indicies, target_batch_indicies = get_training_batch_indicies(index_list, batch_size)\n",
    "\n",
    "  \n",
    "    if len(train_batch_indicies) > 0:\n",
    "        \n",
    "        for i in range(step_len):\n",
    "            \n",
    "            input_batch = data[train_batch_indicies.long()].squeeze()\n",
    "            target_batch = data[target_batch_indicies].squeeze()\n",
    "            \n",
    "           \n",
    "            output, hidden_state = rnn(input_batch, hidden_state)\n",
    "            \n",
    "            \n",
    "            loss = loss_fn(output.view(-1, vocab_size), target_batch.view(-1))\n",
    "            running_loss += loss.item() / step_len\n",
    "            \n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            train_batch_indicies = train_batch_indicies + 1\n",
    "            target_batch_indicies = target_batch_indicies + 1\n",
    "\n",
    "        \n",
    "        print('\\n' + '-' * 75)\n",
    "        print(f\"\\nStep: {step} Loss: {running_loss}\")\n",
    "\n",
    "        \n",
    "        save_dict = {}\n",
    "        \n",
    "        save_dict['state_dict'] = rnn.state_dict()\n",
    "        \n",
    "        save_dict['ix_to_word'] = ix_to_word\n",
    "        save_dict['word_to_ix'] = word_to_ix\n",
    "        \n",
    "        torch.save(save_dict, save_path)\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            \n",
    "            rand_index = np.random.randint(data_size - 1)\n",
    "            input_batch = data[rand_index: rand_index + 1]\n",
    "            hidden_state = None\n",
    "\n",
    "            \n",
    "            for i in range(gen_seq_len):\n",
    "                \n",
    "                output, hidden_state = rnn(input_batch, hidden_state)\n",
    "\n",
    "                \n",
    "                output = F.softmax(torch.squeeze(output), dim=0)\n",
    "                dist = Categorical(output)\n",
    "                index = dist.sample()\n",
    "\n",
    "                \n",
    "                print(ix_to_word[index.item()], end=' ')\n",
    "\n",
    "                \n",
    "                input_batch[0][0] = index.item()\n",
    "    else:\n",
    "        print(f\"\\nStep: {step} - Insufficient data for training.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
